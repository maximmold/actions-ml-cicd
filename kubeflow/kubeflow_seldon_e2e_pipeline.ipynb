{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end Reusable ML Pipeline with Seldon and Kubeflow\n",
    "\n",
    "In this example we showcase how to build re-usable components to build an ML pipeline that can be trained and deployed at scale.\n",
    "\n",
    "We will automate content moderation on the Reddit comments in /r/science building a machine learning NLP model with the following components:\n",
    "\n",
    "![](img/completed-pipeline-deploy.jpg)\n",
    "\n",
    "This tutorial will break down in the following sections:\n",
    "\n",
    "1) Run all the services (Kubeflow and Seldon)\n",
    "\n",
    "2) Test and build all our reusable pipeline steps\n",
    "\n",
    "3) Use Kubeflow to Train the Pipeline and Deploy to Seldon\n",
    "\n",
    "5) Test Seldon Deployed ML REST Endpoints\n",
    "\n",
    "6) Visualise Seldon's Production ML Pipelines\n",
    "\n",
    "## Before you start\n",
    "Make sure you install the following dependencies, as they are critical for this example to work:\n",
    "\n",
    "* Helm v3.0.0+\n",
    "* A Kubernetes cluster running v1.13 or above (minkube / docker-for-windows work well if enough RAM)\n",
    "* kubectl v1.14+\n",
    "* ksonnet v0.13.1+\n",
    "* kfctl 0.5.1 - Please use this exact version as there are major changes every few months\n",
    "* Python 3.6+\n",
    "* Python DEV requirements (we'll install them below)\n",
    "\n",
    "Let's get started! ðŸš€ðŸ”¥ We will be building the end-to-end pipeline below:\n",
    "\n",
    "![](img/kubeflow-seldon-nlp-full.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python-dateutil\r\n",
      "https://storage.googleapis.com/ml-pipeline/release/0.1.20/kfp.tar.gz\r\n",
      "kubernetes\r\n",
      "click\r\n",
      "seldon_core\r\n",
      "numpy\r\n",
      "pandas\r\n",
      "spacy\r\n",
      "sklearn\r\n"
     ]
    }
   ],
   "source": [
    "!cat requirements-dev.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://storage.googleapis.com/ml-pipeline/release/0.1.20/kfp.tar.gz (from -r requirements-dev.txt (line 2))\n",
      "  Using cached https://storage.googleapis.com/ml-pipeline/release/0.1.20/kfp.tar.gz\n",
      "Requirement already satisfied (use --upgrade to upgrade): kfp==0.1.20 from https://storage.googleapis.com/ml-pipeline/release/0.1.20/kfp.tar.gz in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from -r requirements-dev.txt (line 2))\n",
      "Requirement already satisfied: python-dateutil in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from -r requirements-dev.txt (line 1)) (2.8.1)\n",
      "Requirement already satisfied: kubernetes in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from -r requirements-dev.txt (line 3)) (10.0.1)\n",
      "Requirement already satisfied: click in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from -r requirements-dev.txt (line 4)) (7.0)\n",
      "Requirement already satisfied: seldon_core in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from -r requirements-dev.txt (line 5)) (1.0.1)\n",
      "Requirement already satisfied: numpy in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from -r requirements-dev.txt (line 6)) (1.18.1)\n",
      "Requirement already satisfied: pandas in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from -r requirements-dev.txt (line 7)) (0.25.3)\n",
      "Requirement already satisfied: spacy in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from -r requirements-dev.txt (line 8)) (2.2.3)\n",
      "Requirement already satisfied: sklearn in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from -r requirements-dev.txt (line 9)) (0.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.15 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from kfp==0.1.20->-r requirements-dev.txt (line 2)) (1.24.2)\n",
      "Requirement already satisfied: six>=1.10 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from kfp==0.1.20->-r requirements-dev.txt (line 2)) (1.12.0)\n",
      "Requirement already satisfied: certifi in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from kfp==0.1.20->-r requirements-dev.txt (line 2)) (2019.11.28)\n",
      "Requirement already satisfied: PyYAML in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from kfp==0.1.20->-r requirements-dev.txt (line 2)) (5.3)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from kfp==0.1.20->-r requirements-dev.txt (line 2)) (1.25.0)\n",
      "Requirement already satisfied: PyJWT>=1.6.4 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from kfp==0.1.20->-r requirements-dev.txt (line 2)) (1.7.1)\n",
      "Requirement already satisfied: cryptography>=2.4.2 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from kfp==0.1.20->-r requirements-dev.txt (line 2)) (2.8)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from kfp==0.1.20->-r requirements-dev.txt (line 2)) (1.10.1)\n",
      "Requirement already satisfied: requests_toolbelt>=0.8.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from kfp==0.1.20->-r requirements-dev.txt (line 2)) (0.9.1)\n",
      "Requirement already satisfied: kfp-server-api<0.1.19,>=0.1.18 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from kfp==0.1.20->-r requirements-dev.txt (line 2)) (0.1.18.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from kubernetes->-r requirements-dev.txt (line 3)) (0.57.0)\n",
      "Requirement already satisfied: requests in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from kubernetes->-r requirements-dev.txt (line 3)) (2.22.0)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from kubernetes->-r requirements-dev.txt (line 3)) (41.4.0)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from kubernetes->-r requirements-dev.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: protobuf<4.0.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from seldon_core->-r requirements-dev.txt (line 5)) (3.11.2)\n",
      "Requirement already satisfied: minio<6.0.0,>=4.0.9 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from seldon_core->-r requirements-dev.txt (line 5)) (5.0.7)\n",
      "Requirement already satisfied: jaeger-client<4.2.0,>=4.1.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from seldon_core->-r requirements-dev.txt (line 5)) (4.1.0)\n",
      "Requirement already satisfied: redis<4.0.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from seldon_core->-r requirements-dev.txt (line 5)) (3.3.11)\n",
      "Requirement already satisfied: flatbuffers<2.0.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from seldon_core->-r requirements-dev.txt (line 5)) (1.11)\n",
      "Requirement already satisfied: opentracing<2.3.0,>=2.2.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from seldon_core->-r requirements-dev.txt (line 5)) (2.2.0)\n",
      "Requirement already satisfied: grpcio<2.0.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from seldon_core->-r requirements-dev.txt (line 5)) (1.26.0)\n",
      "Requirement already satisfied: grpcio-opentracing<1.2.0,>=1.1.4 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from seldon_core->-r requirements-dev.txt (line 5)) (1.1.4)\n",
      "Requirement already satisfied: gunicorn<20.1.0,>=19.9.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from seldon_core->-r requirements-dev.txt (line 5)) (20.0.4)\n",
      "Requirement already satisfied: azure-storage-blob<3.0.0,>=2.0.1 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from seldon_core->-r requirements-dev.txt (line 5)) (2.1.0)\n",
      "Requirement already satisfied: pyaml<20.0.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from seldon_core->-r requirements-dev.txt (line 5)) (19.12.0)\n",
      "Requirement already satisfied: Flask-cors<4.0.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from seldon_core->-r requirements-dev.txt (line 5)) (3.0.8)\n",
      "Requirement already satisfied: Flask<2.0.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from seldon_core->-r requirements-dev.txt (line 5)) (1.1.1)\n",
      "Requirement already satisfied: Flask-OpenTracing<1.2.0,>=1.1.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from seldon_core->-r requirements-dev.txt (line 5)) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from pandas->-r requirements-dev.txt (line 7)) (2019.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy->-r requirements-dev.txt (line 8)) (3.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy->-r requirements-dev.txt (line 8)) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy->-r requirements-dev.txt (line 8)) (2.0.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy->-r requirements-dev.txt (line 8)) (0.4.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy->-r requirements-dev.txt (line 8)) (1.0.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy->-r requirements-dev.txt (line 8)) (0.6.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy->-r requirements-dev.txt (line 8)) (1.1.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy->-r requirements-dev.txt (line 8)) (1.0.1)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy->-r requirements-dev.txt (line 8)) (7.3.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from sklearn->-r requirements-dev.txt (line 9)) (0.22.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.1.20->-r requirements-dev.txt (line 2)) (0.5.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.1.20->-r requirements-dev.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from cryptography>=2.4.2->kfp==0.1.20->-r requirements-dev.txt (line 2)) (1.13.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.1.20->-r requirements-dev.txt (line 2)) (4.0.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.1.20->-r requirements-dev.txt (line 2)) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.1.20->-r requirements-dev.txt (line 2)) (0.2.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from requests->kubernetes->-r requirements-dev.txt (line 3)) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from requests->kubernetes->-r requirements-dev.txt (line 3)) (3.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from requests-oauthlib->kubernetes->-r requirements-dev.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: configparser in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from minio<6.0.0,>=4.0.9->seldon_core->-r requirements-dev.txt (line 5)) (4.0.2)\n",
      "Requirement already satisfied: threadloop<2,>=1 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from jaeger-client<4.2.0,>=4.1.0->seldon_core->-r requirements-dev.txt (line 5)) (1.0.2)\n",
      "Requirement already satisfied: tornado<6,>=4.3 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from jaeger-client<4.2.0,>=4.1.0->seldon_core->-r requirements-dev.txt (line 5)) (5.1.1)\n",
      "Requirement already satisfied: thrift in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from jaeger-client<4.2.0,>=4.1.0->seldon_core->-r requirements-dev.txt (line 5)) (0.13.0)\n",
      "Requirement already satisfied: azure-common>=1.1.5 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from azure-storage-blob<3.0.0,>=2.0.1->seldon_core->-r requirements-dev.txt (line 5)) (1.1.24)\n",
      "Requirement already satisfied: azure-storage-common~=2.1 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from azure-storage-blob<3.0.0,>=2.0.1->seldon_core->-r requirements-dev.txt (line 5)) (2.1.0)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from Flask<2.0.0->seldon_core->-r requirements-dev.txt (line 5)) (0.16.0)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from Flask<2.0.0->seldon_core->-r requirements-dev.txt (line 5)) (1.1.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from Flask<2.0.0->seldon_core->-r requirements-dev.txt (line 5)) (2.10.3)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy->-r requirements-dev.txt (line 8)) (1.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy->-r requirements-dev.txt (line 8)) (4.36.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from scikit-learn->sklearn->-r requirements-dev.txt (line 9)) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from scikit-learn->sklearn->-r requirements-dev.txt (line 9)) (0.14.1)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.1.20->-r requirements-dev.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.4.2->kfp==0.1.20->-r requirements-dev.txt (line 2)) (2.19)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth>=1.6.1->kfp==0.1.20->-r requirements-dev.txt (line 2)) (0.4.8)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from Jinja2>=2.10.1->Flask<2.0.0->seldon_core->-r requirements-dev.txt (line 5)) (1.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->-r requirements-dev.txt (line 8)) (0.6.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.1.20->-r requirements-dev.txt (line 2)) (1.51.0)\n",
      "Requirement already satisfied: more-itertools in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->-r requirements-dev.txt (line 8)) (8.0.2)\n",
      "Building wheels for collected packages: kfp\n",
      "  Building wheel for kfp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp: filename=kfp-0.1.20-cp37-none-any.whl size=93824 sha256=025a79a3de86736064e25b0e5f5fcd8b4b3d95d45c40abbb19a58d5dce01b755\n",
      "  Stored in directory: /private/var/folders/gb/sc8b_s055d96rpbp8gkmfm440000gn/T/pip-ephem-wheel-cache-38p7a70w/wheels/ae/bb/02/32b1356ee756181099d8f1b0950ac6567cb2b38e71b48f02e8\n",
      "Successfully built kfp\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements-dev.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Run all the services (Kubeflow and Seldon)\n",
    "Kubeflow's CLI allows us to create a project which will allow us to build the configuration we need to deploy our kubeflow and seldon clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: \u001b[33m'kfctl init' has been removed.\u001b[0m\n",
      "Please switch to new semantics.\n",
      "To install run -> \u001b[33mkfctl apply -f ${CONFIG}\u001b[0m\n",
      "For more information, run 'kfctl apply -h' or read the docs at www.kubeflow.org.\n",
      "Usage:\n",
      "  kfctl init [flags]\n",
      "\n",
      "Flags:\n",
      "  -h, --help   help for init\n",
      "\n",
      "\u001b[33m'kfctl init' has been removed.\u001b[0m\n",
      "Please switch to new semantics.\n",
      "To install run -> \u001b[33mkfctl apply -f ${CONFIG}\u001b[0m\n",
      "For more information, run 'kfctl apply -h' or read the docs at www.kubeflow.org.\n",
      "ls: kubeflow-seldon: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!kfctl init kubeflow-seldon\n",
    "!ls kubeflow-seldon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the following commands to basically launch our Kubeflow cluster with all its components. \n",
    "\n",
    "It may take a while to download all the images for Kubeflow so feel free to make yourself a cup of â˜•.\n",
    "\n",
    "If you have a terminal you can see how the containers are created in real-time by running `kubectl get pods -n kubeflow -w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd kubeflow-seldon\n",
    "kfctl generate all -V\n",
    "kfctl apply all -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Seldon Core\n",
    "\n",
    "Use the setup notebook to [Install Seldon Core](../../seldon_core_setup.ipynb#Install-Seldon-Core). Instructions [also online](./seldon_core_setup.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporary fix for Argo image\n",
    "\n",
    "At the time of writing we need to make some updates in the Argo images with the following commands below.\n",
    "\n",
    "(This basically changes the images to the latest ones, otherwise we will get an error when we attach the volume)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.extensions/workflow-controller patched\n",
      "deployment.extensions/ml-pipeline patched\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kubeflow patch deployments. workflow-controller --patch '{\"spec\": {\"template\": {\"spec\": {\"containers\": [{\"name\": \"workflow-controller\", \"image\": \"argoproj/workflow-controller:v2.3.0-rc3\"}]}}}}'\n",
    "!kubectl -n kubeflow patch deployments. ml-pipeline --patch '{\"spec\": {\"template\": {\"spec\": {\"containers\": [{\"name\": \"ml-pipeline-api-server\", \"image\": \"elikatsis/ml-pipeline-api-server:0.1.18-pick-1289\"}]}}}}'\n",
    "# !kubectl -n kubeflow patch configmaps workflow-controller-configmap --patch '{\"data\": {\"config\": \"{ executorImage: argoproj/argoexec:v2.3.0-rc3,artifactRepository:{s3: {bucket: mlpipeline,keyPrefix: artifacts,endpoint: minio-service.kubeflow:9000,insecure: true,accessKeySecret: {name: mlpipeline-minio-artifact,key: accesskey},secretKeySecret: {name: mlpipeline-minio-artifact,key: secretkey}}}}\" }}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last command you need to run actually needs to be manual as the patch cannot change configmap contents directly\n",
    "\n",
    "You need to run the edit commad and change the executorImage to: `argoproj/argoexec:v2.3.0-rc3`\n",
    "\n",
    "The command should be run from a terminal:\n",
    "\n",
    "```\n",
    "kubectl edit configmaps workflow-controller-configmap -n kubeflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Test and build all our reusable pipeline steps\n",
    "\n",
    "We will start by building each of the components in our ML pipeline. \n",
    "\n",
    "![](img/kubeflow-seldon-nlp-reusable-components.jpg)\n",
    "\n",
    "### Let's first have a look at our steps:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mclean_text\u001b[m\u001b[m         \u001b[34mlr_text_classifier\u001b[m\u001b[m \u001b[34mtfidf_vectorizer\u001b[m\u001b[m\r\n",
      "\u001b[34mdata_downloader\u001b[m\u001b[m    \u001b[34mspacy_tokenize\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls pipeline/pipeline_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the steps can be found in the `pipeline/pipeline_steps/` folder, and all have the following structure:\n",
    "* `pipeline_step.py` which exposes the functionality through a CLI \n",
    "* `Transformer.py` which transforms the data accordingly\n",
    "* `requirements.txt` which states the python dependencies to run\n",
    "* `build_image.sh` which uses `s2i` to build the image with one line\n",
    "\n",
    "We can run through each step manually if we want, but it will eventually be packaged into a SeldonDeployment in a single pod and will be run through as a directed acyclic graph.\n",
    "\n",
    "#### Install the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r pipeline/pipeline_steps/data_downloader/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a temporary directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: tmp: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the data downloaded step with parameters to pull down from appropriate location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python pipeline/pipeline_steps/data_downloader/pipeline_step.py --labels-path tmp/labels.data --features-path tmp/text.data --csv-url https://raw.githubusercontent.com/axsauze/reddit-classification-exploration/master/data/reddit_train.csv --csv-encoding ISO-8859-1 --features-column BODY --labels-column REMOVED "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dill==0.2.9 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from -r pipeline/pipeline_steps/clean_text/requirements.txt (line 1)) (0.2.9)\n",
      "Requirement already satisfied: click in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from -r pipeline/pipeline_steps/clean_text/requirements.txt (line 2)) (7.0)\n",
      "Requirement already satisfied: numpy in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from -r pipeline/pipeline_steps/clean_text/requirements.txt (line 3)) (1.16.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r pipeline/pipeline_steps/clean_text/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:root:[\"Always be wary of news articles that cite unpublished studies. Even if they are eventually published as claimed, it's not the responsible way to report on science.\\r\\n\\r\\nIt could be an absolutely shit study, but once the claims are made in public you can't unring the bell. At least if the study is reported on *when* it is published, one can challenge it at the same time. \"\n",
      " 'The problem I have with this is that the article appears to credit the plain packets with the decline in smoking. \\r\\n\\r\\n\"Between 2010 and 2013, the proportion of daily smokers in Australia dropped from 15.1 to 12.8 per cent - a record decline.\" In other words, a drop of 2.3 percent (15.1 - 12.8). \\r\\n\\r\\nYet in the United States - which does *not* have plain packaging - the rate dropped from 19.3 (in 2010) to 17.8 (in 2013). That\\'s 1.5 percent (19.3 - 17.8). Granted, that\\'s not as much as Australia\\'s decline but it indicates that you cannot credit Australia\\'s entire decline to the plain packaging since declines happened in places without plain packaging.\\r\\n\\r\\n'\n",
      " \"This is indicative of a typical power law, and is found in any number of areas.  I'm actually a bit surprised that it is only half.\"\n",
      " ...\n",
      " 'Has there been any such studies with Adderall?\\r\\n\\r\\nAs a parent who just gave his 12 year old his first dose of Adderall yesterday, this scares me.\\r\\n\\r\\nAnecdotally, the changes I saw in him were amazing. \\r\\n\\r\\n'\n",
      " 'VR will be widely used for so many \"*phobias\" the face your fear ones, as of Acrophobia, Claustrophobia, Aviatophobia, Hemophobia , Arachnophobia, Cynophobia, Ophidiophobia even Dentophobia :D'\n",
      " 'I wonder how strong this correlation is in countries with universal health care.']\n",
      "WARNING:root:[\"Always be wary of news articles that cite unpublished studies. Even if they are eventually published as claimed, it's not the responsible way to report on science.  It could be an absolutely shit study, but once the claims are made in public you can't unring the bell. At least if the study is reported on *when* it is published, one can challenge it at the same time. \"\n",
      " 'The problem I have with this is that the article appears to credit the plain packets with the decline in smoking.   \"Between 2010 and 2013, the proportion of daily smokers in Australia dropped from 15.1 to 12.8 per cent - a record decline.\" In other words, a drop of 2.3 percent (15.1 - 12.8).   Yet in the United States - which does *not* have plain packaging - the rate dropped from 19.3 (in 2010) to 17.8 (in 2013). That\\'s 1.5 percent (19.3 - 17.8). Granted, that\\'s not as much as Australia\\'s decline but it indicates that you cannot credit Australia\\'s entire decline to the plain packaging since declines happened in places without plain packaging.  '\n",
      " \"This is indicative of a typical power law, and is found in any number of areas.  I'm actually a bit surprised that it is only half.\"\n",
      " ...\n",
      " 'Has there been any such studies with Adderall?  As a parent who just gave his 12 year old his first dose of Adderall yesterday, this scares me.  Anecdotally, the changes I saw in him were amazing.   '\n",
      " 'VR will be widely used for so many \"*phobias\" the face your fear ones, as of Acrophobia, Claustrophobia, Aviatophobia, Hemophobia , Arachnophobia, Cynophobia, Ophidiophobia even Dentophobia :D'\n",
      " 'I wonder how strong this correlation is in countries with universal health care.']\n"
     ]
    }
   ],
   "source": [
    "!python pipeline/pipeline_steps/clean_text/pipeline_step.py --in-path tmp/text.data --out-path tmp/clean.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually a very simple file, as we are using the click library to define the commands:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import dill\r\n",
      "import click\r\n",
      "import dill\r\n",
      "try:\r\n",
      "    # Running for tests\r\n",
      "    from .Transformer import Transformer\r\n",
      "except:\r\n",
      "    # Running from CLI\r\n",
      "    from Transformer import Transformer\r\n",
      "\r\n",
      "@click.command()\r\n",
      "@click.option('--in-path', default=\"/mnt/raw_text.data\")\r\n",
      "@click.option('--out-path', default=\"/mnt/clean_text.data\")\r\n",
      "def run_pipeline(in_path, out_path):\r\n",
      "    clean_text_transformer = Transformer()\r\n",
      "    with open(in_path, 'rb') as in_f:\r\n",
      "        x = dill.load(in_f)\r\n",
      "    y = clean_text_transformer.predict(x)\r\n",
      "    with open(out_path, \"wb\") as out_f:\r\n",
      "        dill.dump(y, out_f)\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "    run_pipeline()\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat pipeline/pipeline_steps/clean_text/pipeline_step.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformer is where the data munging and transformation stage comes in, which will be wrapped by the container and exposed through the Seldon Engine to ensure our pipeline can be used in production.\n",
    "\n",
    "Seldon provides multiple different features, such as abilities to send custom metrics, pre-process / post-process data and more. In this example we will only be exposing the `predict` step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import re \r\n",
      "from html.parser import HTMLParser\r\n",
      "import numpy as np\r\n",
      "import logging\r\n",
      "\r\n",
      "class Transformer():\r\n",
      "    __html_parser = HTMLParser()\r\n",
      "    __uplus_pattern = \\\r\n",
      "        re.compile(\"\\<[uU]\\+(?P<digit>[a-zA-Z0-9]+)\\>\")\r\n",
      "    __markup_link_pattern = \\\r\n",
      "        re.compile(\"\\[(.*)\\]\\((.*)\\)\")\r\n",
      "\r\n",
      "    def predict(self, X, feature_names=[]):\r\n",
      "        logging.warning(X)\r\n",
      "        f = np.vectorize(Transformer.transform_clean_text)\r\n",
      "        X_clean = f(X)\r\n",
      "        logging.warning(X_clean)\r\n",
      "        return X_clean\r\n",
      "\r\n",
      "    def fit(self, X, y=None, **fit_params):\r\n",
      "        return self\r\n",
      "    \r\n",
      "    @staticmethod\r\n",
      "    def transform_clean_text(raw_text):\r\n",
      "        try:\r\n",
      "            decoded = raw_text.encode(\"ISO-8859-1\").decode(\"utf-8\")\r\n",
      "        except:\r\n",
      "            decoded = raw_text.encode(\"ISO-8859-1\").decode(\"cp1252\")\r\n",
      "        html_unescaped =Transformer.\\\r\n",
      "            __html_parser.unescape(decoded) \r\n",
      "        html_unescaped = re.sub(r\"\\r\\n\", \" \", html_unescaped)\r\n",
      "        html_unescaped = re.sub(r\"\\r\\r\\n\", \" \", html_unescaped)\r\n",
      "        html_unescaped = re.sub(r\"\\r\", \" \", html_unescaped)\r\n",
      "        html_unescaped = html_unescaped.replace(\"&gt;\", \" > \")\r\n",
      "        html_unescaped = html_unescaped.replace(\"&lt;\", \" < \")\r\n",
      "        html_unescaped = html_unescaped.replace(\"--\", \" - \")\r\n",
      "        html_unescaped = Transformer.__uplus_pattern.sub(\r\n",
      "            \" U\\g<digit> \", html_unescaped)\r\n",
      "        html_unescaped = Transformer.__markup_link_pattern.sub(\r\n",
      "            \" \\1 \\2 \", html_unescaped)\r\n",
      "        html_unescaped = html_unescaped.replace(\"\\\\\", \"\")\r\n",
      "        return html_unescaped\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat pipeline/pipeline_steps/clean_text/Transformer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dill==0.2.9 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from -r pipeline/pipeline_steps/spacy_tokenize/requirements.txt (line 1)) (0.2.9)\n",
      "Requirement already satisfied: click in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from -r pipeline/pipeline_steps/spacy_tokenize/requirements.txt (line 2)) (7.0)\n",
      "Requirement already satisfied: numpy in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from -r pipeline/pipeline_steps/spacy_tokenize/requirements.txt (line 3)) (1.16.3)\n",
      "Requirement already satisfied: spacy in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from -r pipeline/pipeline_steps/spacy_tokenize/requirements.txt (line 4)) (2.2.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy->-r pipeline/pipeline_steps/spacy_tokenize/requirements.txt (line 4)) (2.0.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy->-r pipeline/pipeline_steps/spacy_tokenize/requirements.txt (line 4)) (0.6.0)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy->-r pipeline/pipeline_steps/spacy_tokenize/requirements.txt (line 4)) (7.3.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy->-r pipeline/pipeline_steps/spacy_tokenize/requirements.txt (line 4)) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy->-r pipeline/pipeline_steps/spacy_tokenize/requirements.txt (line 4)) (41.4.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy->-r pipeline/pipeline_steps/spacy_tokenize/requirements.txt (line 4)) (1.1.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy->-r pipeline/pipeline_steps/spacy_tokenize/requirements.txt (line 4)) (1.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy->-r pipeline/pipeline_steps/spacy_tokenize/requirements.txt (line 4)) (2.22.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy->-r pipeline/pipeline_steps/spacy_tokenize/requirements.txt (line 4)) (1.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy->-r pipeline/pipeline_steps/spacy_tokenize/requirements.txt (line 4)) (3.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy->-r pipeline/pipeline_steps/spacy_tokenize/requirements.txt (line 4)) (1.0.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy->-r pipeline/pipeline_steps/spacy_tokenize/requirements.txt (line 4)) (4.36.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r pipeline/pipeline_steps/spacy_tokenize/requirements.txt (line 4)) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r pipeline/pipeline_steps/spacy_tokenize/requirements.txt (line 4)) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r pipeline/pipeline_steps/spacy_tokenize/requirements.txt (line 4)) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r pipeline/pipeline_steps/spacy_tokenize/requirements.txt (line 4)) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy->-r pipeline/pipeline_steps/spacy_tokenize/requirements.txt (line 4)) (1.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->-r pipeline/pipeline_steps/spacy_tokenize/requirements.txt (line 4)) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->-r pipeline/pipeline_steps/spacy_tokenize/requirements.txt (line 4)) (8.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r pipeline/pipeline_steps/spacy_tokenize/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.2.5\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12.0MB 4.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from en_core_web_sm==2.2.5) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.16.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.22.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.3.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.1)\n",
      "Requirement already satisfied: setuptools in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (41.4.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.36.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (8.0.2)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.2.5-cp37-none-any.whl size=12011740 sha256=f436ed6dbbb1b7b379ba936dd925448e099fbd441d50aab90afc868093853cd3\n",
      "  Stored in directory: /private/var/folders/gb/sc8b_s055d96rpbp8gkmfm440000gn/T/pip-ephem-wheel-cache-xf6n9402/wheels/6a/47/fb/6b5a0b8906d8e8779246c67d4658fd8a544d4a03a75520197a\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.2.5\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:root:[\"Always be wary of news articles that cite unpublished studies. Even if they are eventually published as claimed, it's not the responsible way to report on science.  It could be an absolutely shit study, but once the claims are made in public you can't unring the bell. At least if the study is reported on *when* it is published, one can challenge it at the same time. \"\n",
      " 'The problem I have with this is that the article appears to credit the plain packets with the decline in smoking.   \"Between 2010 and 2013, the proportion of daily smokers in Australia dropped from 15.1 to 12.8 per cent - a record decline.\" In other words, a drop of 2.3 percent (15.1 - 12.8).   Yet in the United States - which does *not* have plain packaging - the rate dropped from 19.3 (in 2010) to 17.8 (in 2013). That\\'s 1.5 percent (19.3 - 17.8). Granted, that\\'s not as much as Australia\\'s decline but it indicates that you cannot credit Australia\\'s entire decline to the plain packaging since declines happened in places without plain packaging.  '\n",
      " \"This is indicative of a typical power law, and is found in any number of areas.  I'm actually a bit surprised that it is only half.\"\n",
      " ...\n",
      " 'Has there been any such studies with Adderall?  As a parent who just gave his 12 year old his first dose of Adderall yesterday, this scares me.  Anecdotally, the changes I saw in him were amazing.   '\n",
      " 'VR will be widely used for so many \"*phobias\" the face your fear ones, as of Acrophobia, Claustrophobia, Aviatophobia, Hemophobia , Arachnophobia, Cynophobia, Ophidiophobia even Dentophobia :D'\n",
      " 'I wonder how strong this correlation is in countries with universal health care.']\n",
      "WARNING:root:[list(['always', 'be', 'wary', 'of', 'news', 'article', 'that', 'cite', 'unpublished', 'study', 'even', 'if', 'they', 'be', 'eventually', 'publish', 'a', 'claim', '-pron-', 'have', 'not', 'the', 'responsible', 'way', 'to', 'report', 'on', 'science', 'it', 'can', 'be', 'a', 'absolutely', 'shit', 'study', 'but', 'once', 'the', 'claim', 'be', 'make', 'in', 'public', 'you', 'can', 'not', 'unring', 'the', 'bell', 'at', 'little', 'if', 'the', 'study', 'be', 'report', 'on', 'when', 'it', 'be', 'publish', 'one', 'can', 'challenge', 'it', 'at', 'the', 'same', 'time'])\n",
      " list(['the', 'problem', 'i', 'have', 'with', 'this', 'be', 'that', 'the', 'article', 'appear', 'to', 'credit', 'the', 'plain', 'packet', 'with', 'the', 'decline', 'in', 'smoke', 'between', '2010', 'and', '2013', 'the', 'proportion', 'of', 'daily', 'smoker', 'in', 'australia', 'drop', 'from', '15.1', 'to', '12.8', 'per', 'cent', 'a', 'record', 'decline', 'in', 'other', 'word', 'a', 'drop', 'of', '2.3', 'percent', '15.1', '12.8', 'yet', 'in', 'the', 'united', 'states', 'which', 'doe', 'not', 'have', 'plain', 'package', 'the', 'rate', 'drop', 'from', '19.3', 'in', '2010', 'to', '17.8', 'in', '2013', 'that', 'have', '1.5', 'percent', '19.3', '17.8', 'granted', 'that', 'have', 'not', 'a', 'much', 'a', 'australia', 'have', 'decline', 'but', 'it', 'indicate', 'that', 'you', 'can', 'not', 'credit', 'australia', 'have', 'entire', 'decline', 'to', 'the', 'plain', 'package', 'since', 'decline', 'happen', 'in', 'place', 'without', 'plain', 'package'])\n",
      " list(['this', 'be', 'indicative', 'of', 'a', 'typical', 'power', 'law', 'and', 'be', 'find', 'in', 'any', 'numb', 'of', 'area', '-pron-', 'be', 'actually', 'a', 'bite', 'surprise', 'that', 'it', 'be', 'only', 'half'])\n",
      " ...\n",
      " list(['has', 'there', 'be', 'any', 'such', 'study', 'with', 'adderall', 'as', 'a', 'parent', 'who', 'just', 'give', 'his', '12', 'year', 'old', 'his', '\\ufeff1', 'dose', 'of', 'adderall', 'yesterday', 'this', 'scare', 'me', 'anecdotally', 'the', 'change', 'i', 'see', 'in', 'him', 'be', 'amaze'])\n",
      " list(['vr', 'will', 'be', 'widely', 'use', 'for', 'so', 'many', 'phobia', 'the', 'face', 'your', 'fear', 'one', 'a', 'of', 'acrophobia', 'claustrophobia', 'aviatophobia', 'hemophobia', 'arachnophobia', 'cynophobia', 'ophidiophobia', 'even', 'dentophobia', ':d'])\n",
      " list(['i', 'wonder', 'how', 'strong', 'this', 'correlation', 'be', 'in', 'country', 'with', 'universal', 'health', 'care'])]\n"
     ]
    }
   ],
   "source": [
    "!cd pipeline/pipeline_steps/spacy_tokenize && python pipeline_step.py --in-path ../../../tmp/clean.data --out-path ../../../tmp/tokens.data && cd ../../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dill==0.2.9 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from -r pipeline/pipeline_steps/tfidf_vectorizer/requirements.txt (line 1)) (0.2.9)\n",
      "Requirement already satisfied: click in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from -r pipeline/pipeline_steps/tfidf_vectorizer/requirements.txt (line 2)) (7.0)\n",
      "Requirement already satisfied: numpy==1.16.3 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from -r pipeline/pipeline_steps/tfidf_vectorizer/requirements.txt (line 3)) (1.16.3)\n",
      "Collecting scikit-learn==0.20.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/1a/5dc99dfa7e7bf2e1f22a07f440b61299148a42d13dee1d5153360313ed67/scikit_learn-0.20.3-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (8.0MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8.0MB 1.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.13.3 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from scikit-learn==0.20.3->-r pipeline/pipeline_steps/tfidf_vectorizer/requirements.txt (line 4)) (1.4.1)\n",
      "Installing collected packages: scikit-learn\n",
      "  Found existing installation: scikit-learn 0.22.1\n",
      "    Uninstalling scikit-learn-0.22.1:\n",
      "      Successfully uninstalled scikit-learn-0.22.1\n",
      "Successfully installed scikit-learn-0.20.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -r pipeline/pipeline_steps/tfidf_vectorizer/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python pipeline/pipeline_steps/tfidf_vectorizer/pipeline_step.py --in-path tmp/tokens.data --out-path tmp/tfidf.data --max-features 10000 --ngram-range 3 --action train --model-path tmp/tfidf.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dill==0.2.9 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from -r pipeline/pipeline_steps/lr_text_classifier/requirements.txt (line 1)) (0.2.9)\r\n",
      "Requirement already satisfied: click in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from -r pipeline/pipeline_steps/lr_text_classifier/requirements.txt (line 2)) (7.0)\r\n",
      "Requirement already satisfied: numpy==1.16.3 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from -r pipeline/pipeline_steps/lr_text_classifier/requirements.txt (line 3)) (1.16.3)\r\n",
      "Requirement already satisfied: scikit-learn==0.20.3 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from -r pipeline/pipeline_steps/lr_text_classifier/requirements.txt (line 4)) (0.20.3)\r\n",
      "Requirement already satisfied: scipy>=0.13.3 in /Users/maximmoldenhauer/miniconda3/lib/python3.7/site-packages (from scikit-learn==0.20.3->-r pipeline/pipeline_steps/lr_text_classifier/requirements.txt (line 4)) (1.4.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -r pipeline/pipeline_steps/lr_text_classifier/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python pipeline/pipeline_steps/lr_text_classifier/pipeline_step.py --in-path tmp/tfidf.data --labels-path tmp/labels.data --out-path tmp/prediction.data --c-param 0.1 --action train --model-path tmp/lr.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you have ran through all of the pipeline steps, but to run the full thing in sequence, you have to build these as a bunch of docker containers and package in a pod as a SeldonDeployment.\n",
    "\n",
    "If you want to further understand how the CLI pipeline talks to each other, have a look at the end to end test in `pipeline/pipeline_tests/`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: pytest: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!pytest ./pipeline/pipeline_tests/. --disable-pytest-warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the image we provide a build script in each of the steps that contains the instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "\r\n",
      "s2i build . seldonio/seldon-core-s2i-python3:0.6 clean_text_transformer:0.1\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat pipeline/pipeline_steps/clean_text/build_image.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing you need to make sure is that Seldon knows how to wrap the right model and file.\n",
    "\n",
    "This can be achieved with the s2i/environment file. \n",
    "\n",
    "As you can see, here we just tell it we want it to use our `Transformer.py` file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_NAME=Transformer\r\n",
      "API_TYPE=REST\r\n",
      "SERVICE_TYPE=MODEL\r\n",
      "PERSISTENCE=0\r\n"
     ]
    }
   ],
   "source": [
    "!cat pipeline/pipeline_steps/clean_text/.s2i/environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is defined, the only thing we need to do is to run the `build_image.sh` for all the reusable components.\n",
    "\n",
    "Here we show the manual way to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# we must be in the same directory\n",
    "cd pipeline/pipeline_steps/clean_text/ && ./build_image.sh\n",
    "cd ../data_downloader && ./build_image.sh\n",
    "cd ../lr_text_classifier && ./build_image.sh\n",
    "cd ../spacy_tokenize && ./build_image.sh\n",
    "cd ../tfidf_vectorizer && ./build_image.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Train our NLP Pipeline through the Kubeflow UI\n",
    "We can access the Kubeflow dashboard to train our ML pipeline via http://localhost/_/pipeline-dashboard\n",
    "\n",
    "If you can't edit this, you need to make sure that the ambassador gateway service is accessible:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME         TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\r\n",
      "ambassador   NodePort   10.97.236.196   <none>        80:30209/TCP   8m58s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get svc ambassador -n kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my case, I need to change the kind from `NodePort` into `LoadBalancer` which can be done with the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service/ambassador patched\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl patch svc ambassador --type='json' -p '[{\"op\":\"replace\",\"path\":\"/spec/type\",\"value\":\"LoadBalancer\"}]' -n kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've changed it to a loadbalancer, it has allocated the external IP as my localhost so I can access it at http://localhost/_/pipeline-dashboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\r\n",
      "ambassador   LoadBalancer   10.97.236.196   localhost     80:30209/TCP   9m20s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get svc ambassador -n kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this was successfull, you should be able to access the dashboard\n",
    "![](img/k-pipeline-dashboard.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the pipeline\n",
    "Now we want to generate the pipeline. For this we can use the DSL provided by kubeflow to define the actual steps required. \n",
    "\n",
    "The pipeline will look as follows:\n",
    "\n",
    "![](img/kubeflow-seldon-nlp-ml-pipelines.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "import kfp.dsl as dsl\r\n",
      "import yaml\r\n",
      "from kubernetes import client as k8s\r\n",
      "\r\n",
      "\r\n",
      "@dsl.pipeline(\r\n",
      "  name='NLP',\r\n",
      "  description='A pipeline demonstrating reproducible steps for NLP'\r\n",
      ")\r\n",
      "def nlp_pipeline(\r\n",
      "        csv_url=\"https://raw.githubusercontent.com/axsauze/reddit-classification-exploration/master/data/reddit_train.csv\",\r\n",
      "        csv_encoding=\"ISO-8859-1\",\r\n",
      "        features_column=\"BODY\",\r\n",
      "        labels_column=\"REMOVED\",\r\n",
      "        raw_text_path='/mnt/text.data',\r\n",
      "        labels_path='/mnt/labels.data',\r\n",
      "        clean_text_path='/mnt/clean.data',\r\n",
      "        spacy_tokens_path='/mnt/tokens.data',\r\n",
      "        tfidf_vectors_path='/mnt/tfidf.data',\r\n",
      "        lr_prediction_path='/mnt/prediction.data',\r\n",
      "        tfidf_model_path='/mnt/tfidf.model',\r\n",
      "        lr_model_path='/mnt/lr.model',\r\n",
      "        lr_c_param=0.1,\r\n",
      "        tfidf_max_features=10000,\r\n",
      "        tfidf_ngram_range=3,\r\n",
      "        batch_size='100'):\r\n",
      "    \"\"\"\r\n",
      "    Pipeline \r\n",
      "    \"\"\"\r\n",
      "    vop = dsl.VolumeOp(\r\n",
      "      name='my-pvc',\r\n",
      "      resource_name=\"my-pvc\",\r\n",
      "      modes=[\"ReadWriteMany\"],\r\n",
      "      size=\"1Gi\"\r\n",
      "    )\r\n",
      "\r\n",
      "    download_step = dsl.ContainerOp(\r\n",
      "        name='data_downloader',\r\n",
      "        image='data_downloader:0.1',\r\n",
      "        command=\"python\",\r\n",
      "        arguments=[\r\n",
      "            \"/microservice/pipeline_step.py\",\r\n",
      "            \"--labels-path\", labels_path,\r\n",
      "            \"--features-path\", raw_text_path,\r\n",
      "            \"--csv-url\", csv_url,\r\n",
      "            \"--csv-encoding\", csv_encoding,\r\n",
      "            \"--features-column\", features_column,\r\n",
      "            \"--labels-column\", labels_column\r\n",
      "        ],\r\n",
      "        pvolumes={\"/mnt\": vop.volume}\r\n",
      "    )\r\n",
      "\r\n",
      "    clean_step = dsl.ContainerOp(\r\n",
      "        name='clean_text',\r\n",
      "        image='clean_text_transformer:0.1',\r\n",
      "        command=\"python\",\r\n",
      "        arguments=[\r\n",
      "            \"/microservice/pipeline_step.py\",\r\n",
      "            \"--in-path\", raw_text_path,\r\n",
      "            \"--out-path\", clean_text_path,\r\n",
      "        ],\r\n",
      "        pvolumes={\"/mnt\": download_step.pvolume}\r\n",
      "    )\r\n",
      "\r\n",
      "    tokenize_step = dsl.ContainerOp(\r\n",
      "        name='tokenize',\r\n",
      "        image='spacy_tokenizer:0.1',\r\n",
      "        command=\"python\",\r\n",
      "        arguments=[\r\n",
      "            \"/microservice/pipeline_step.py\",\r\n",
      "            \"--in-path\", clean_text_path,\r\n",
      "            \"--out-path\", spacy_tokens_path,\r\n",
      "        ],\r\n",
      "        pvolumes={\"/mnt\": clean_step.pvolume}\r\n",
      "    )\r\n",
      "\r\n",
      "    vectorize_step = dsl.ContainerOp(\r\n",
      "        name='vectorize',\r\n",
      "        image='tfidf_vectorizer:0.1',\r\n",
      "        command=\"python\",\r\n",
      "        arguments=[\r\n",
      "            \"/microservice/pipeline_step.py\",\r\n",
      "            \"--in-path\", spacy_tokens_path,\r\n",
      "            \"--out-path\", tfidf_vectors_path,\r\n",
      "            \"--max-features\", tfidf_max_features,\r\n",
      "            \"--ngram-range\", tfidf_ngram_range,\r\n",
      "            \"--action\", \"train\",\r\n",
      "            \"--model-path\", tfidf_model_path,\r\n",
      "        ],\r\n",
      "        pvolumes={\"/mnt\": tokenize_step.pvolume}\r\n",
      "    )\r\n",
      "\r\n",
      "    predict_step = dsl.ContainerOp(\r\n",
      "        name='predictor',\r\n",
      "        image='lr_text_classifier:0.1',\r\n",
      "        command=\"python\",\r\n",
      "        arguments=[\r\n",
      "            \"/microservice/pipeline_step.py\",\r\n",
      "            \"--in-path\", tfidf_vectors_path,\r\n",
      "            \"--labels-path\", labels_path,\r\n",
      "            \"--out-path\", lr_prediction_path,\r\n",
      "            \"--c-param\", lr_c_param,\r\n",
      "            \"--action\", \"train\",\r\n",
      "            \"--model-path\", lr_model_path,\r\n",
      "        ],\r\n",
      "        pvolumes={\"/mnt\": vectorize_step.pvolume}\r\n",
      "    )\r\n",
      "\r\n",
      "    try:\r\n",
      "        seldon_config = yaml.load(open(\"../deploy_pipeline/seldon_production_pipeline.yaml\"))\r\n",
      "    except:\r\n",
      "        # If this file is run from the project core directory \r\n",
      "        seldon_config = yaml.load(open(\"deploy_pipeline/seldon_production_pipeline.yaml\"))\r\n",
      "\r\n",
      "    deploy_step = dsl.ResourceOp(\r\n",
      "        name=\"seldondeploy\",\r\n",
      "        k8s_resource=seldon_config,\r\n",
      "        attribute_outputs={\"name\": \"{.metadata.name}\"})\r\n",
      "\r\n",
      "    deploy_step.after(predict_step)\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "  import kfp.compiler as compiler\r\n",
      "  compiler.Compiler().compile(nlp_pipeline, __file__ + '.tar.gz')\r\n"
     ]
    }
   ],
   "source": [
    "!cat train_pipeline/nlp_pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking down the  code\n",
    "As you can see in the DSL, we have the ContainerOp - each of those is a step in the Kubeflow pipeline.\n",
    "\n",
    "At the end we can see the `seldondeploy` step which basically deploys the trained pipeline\n",
    "\n",
    "The definition of the SeldonDeployment graph is provided in the `deploy_pipeline/seldon_production_pipeline.yaml` file.\n",
    "\n",
    "The seldondeployment file defines our production execution graph using the same reusable components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\r\n",
      "apiVersion: machinelearning.seldon.io/v1alpha2\r\n",
      "kind: SeldonDeployment\r\n",
      "metadata:\r\n",
      "  labels:\r\n",
      "    app: seldon\r\n",
      "  name: \"seldon-deployment-{{workflow.name}}\"\r\n",
      "  namespace: kubeflow\r\n",
      "spec:\r\n",
      "  annotations:\r\n",
      "    project_name: NLP Pipeline\r\n",
      "    deployment_version: v1\r\n",
      "  name: \"seldon-deployment-{{workflow.name}}\"\r\n",
      "  oauth_key: oauth-key\r\n",
      "  oauth_secret: oauth-secret\r\n",
      "  predictors:\r\n",
      "  - componentSpecs:\r\n",
      "    - spec:\r\n",
      "        containers:\r\n",
      "        - image: clean_text_transformer:0.1\r\n",
      "          imagePullPolicy: IfNotPresent\r\n",
      "          name: cleantext\r\n",
      "          resources:\r\n",
      "            requests:\r\n",
      "              memory: 1Mi\r\n",
      "        - image: spacy_tokenizer:0.1\r\n",
      "          imagePullPolicy: IfNotPresent\r\n",
      "          name: spacytokenizer\r\n",
      "        - image: tfidf_vectorizer:0.1\r\n",
      "          imagePullPolicy: IfNotPresent\r\n",
      "          name: tfidfvectorizer\r\n",
      "          volumeMounts:\r\n",
      "          - name: mypvc\r\n",
      "            mountPath: /mnt\r\n",
      "        - image: lr_text_classifier:0.1\r\n",
      "          imagePullPolicy: IfNotPresent\r\n",
      "          name: lrclassifier\r\n",
      "          volumeMounts:\r\n",
      "          - name: mypvc\r\n",
      "            mountPath: /mnt\r\n",
      "        terminationGracePeriodSeconds: 20\r\n",
      "        volumes:\r\n",
      "        - name: mypvc\r\n",
      "          persistentVolumeClaim:\r\n",
      "            claimName: \"{{workflow.name}}-my-pvc\"\r\n",
      "    graph:\r\n",
      "      children:\r\n",
      "      - name: spacytokenizer\r\n",
      "        endpoint:\r\n",
      "          type: REST\r\n",
      "        type: MODEL\r\n",
      "        children:\r\n",
      "        - name: tfidfvectorizer\r\n",
      "          endpoint:\r\n",
      "            type: REST\r\n",
      "          type: MODEL\r\n",
      "          children:\r\n",
      "          - name: lrclassifier\r\n",
      "            endpoint:\r\n",
      "              type: REST\r\n",
      "            type: MODEL\r\n",
      "            children: []\r\n",
      "      name: cleantext\r\n",
      "      endpoint:\r\n",
      "        type: REST\r\n",
      "      type: MODEL\r\n",
      "    name: single-model\r\n",
      "    replicas: 1\r\n",
      "    annotations:\r\n",
      "      predictor_version: v1\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat deploy_pipeline/seldon_production_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seldon Production pipeline contents\n",
    "If we look at the file we'll be using to deploy our pipeline, we can see that it has the following key points:\n",
    "\n",
    "1) Reusable components definitions as containerSpecs: cleantext, spacytokenizer, tfidfvectorizer & lrclassifier\n",
    "\n",
    "2) DAG (directed acyclic graph) definition for REST pipeline: cleantext -> spacytokenizer -> tfidfvectorizer -> lrclassifier\n",
    "\n",
    "This graph in our production deployment looks as follows:\n",
    "\n",
    "![](img/kubeflow-seldon-nlp-ml-pipelines-deploy.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the pipeline files to upload to Kubeflow\n",
    "To generate the pipeline we just have to run the pipeline file, which will output the `tar.gz` file that will be uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp_pipeline.py\n",
      "nlp_pipeline.py.tar.gz\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Generating graph definition\n",
    "python train_pipeline/nlp_pipeline.py\n",
    "ls train_pipeline/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Run the pipeline\n",
    "\n",
    "We now need to upload the resulting `nlp_pipeline.py.tar.gz` file generated.\n",
    "\n",
    "This can be done through the \"Upload PIpeline\" button in the UI at http://localhost/_/pipeline-dashboard.\n",
    "\n",
    "Once it's uploaded, we want to create and trigger a run! You should now be able to see how each step is executed:\n",
    "\n",
    "![](img/running-pipeline.jpg)\n",
    "\n",
    "### Inspecting the data created in the Persistent Volume\n",
    "The pipeline saves the output of the pipeline together with the trained model in the persistent volume claim.\n",
    "\n",
    "The persistent volume claim is the same name as the argo workflow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME        AGE\r\n",
      "nlp-bddff   2m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get workflow -n kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our workflow is there! So we can actually access it by running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp-bddff"
     ]
    }
   ],
   "source": [
    "!kubectl get workflow -n kubeflow -o jsonpath='{.items[0].metadata.name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can use good old `sed` to insert this workflow name in our PVC-Access controler which we can use to inspect the contents of the volume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: v1\r\n",
      "kind: Pod\r\n",
      "metadata:\r\n",
      "  name: pvc-access-container\r\n",
      "spec:\r\n",
      "  containers:\r\n",
      "  - name: pvc-access-container\r\n",
      "    image: busybox\r\n",
      "    command: [\"/bin/sh\", \"-ec\", \"sleep 1000\"]\r\n",
      "    volumeMounts:\r\n",
      "    - name: mypvc\r\n",
      "      mountPath: /mnt\r\n",
      "  volumes:\r\n",
      "  - name: mypvc\r\n",
      "    persistentVolumeClaim:\r\n",
      "      claimName: nlp-b7qt8-my-pvc\r\n"
     ]
    }
   ],
   "source": [
    "!sed \"s/PVC_NAME/\"$(kubectl get workflow -n kubeflow -o jsonpath='{.items[0].metadata.name}')\"-my-pvc/g\" deploy_pipeline/pvc-access.yaml "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just need to apply this container with our kubectl command, and we can use it to inspect the mounted folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/pvc-access-container created\r\n"
     ]
    }
   ],
   "source": [
    "!sed \"s/PVC_NAME/\"$(kubectl get workflow -n kubeflow -o jsonpath='{.items[0].metadata.name}')\"-my-pvc/g\" deploy_pipeline/pvc-access.yaml | kubectl -n kubeflow apply -f -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                   READY   STATUS    RESTARTS   AGE\r\n",
      "pvc-access-container   1/1     Running   0          6s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods -n kubeflow pvc-access-container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run an `ls` command to see what's inside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32mclean.data\u001b[m       \u001b[1;32mlr.model\u001b[m         \u001b[1;32mtext.data\u001b[m        \u001b[1;32mtfidf.model\u001b[m\r\n",
      "\u001b[1;32mlabels.data\u001b[m      \u001b[1;32mprediction.data\u001b[m  \u001b[1;32mtfidf.data\u001b[m       \u001b[1;32mtokens.data\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kubeflow exec -it pvc-access-container ls /mnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"pvc-access-container\" deleted\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete -f deploy_pipeline/pvc-access.yaml -n kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Test Deployed ML REST Endpoints\n",
    "Now that it's running we have a production ML text pipeline that we can Query using REST and GRPC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can check if our Seldon deployment is running with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                          AGE\r\n",
      "seldon-deployment-nlp-b7qt8   57m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kubeflow get seldondeployment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need the Seldon Pipeline Deployment name to reach the API, so we can get it using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seldon-deployment-nlp-b7qt8"
     ]
    }
   ],
   "source": [
    "!kubectl -n kubeflow get seldondeployment -o jsonpath='{.items[0].metadata.name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can interact with our API in two ways: \n",
    "\n",
    "1) Using CURL or any client like PostMan\n",
    "\n",
    "2) Using the Python SeldonClient\n",
    "\n",
    "### Using CURL from the terminal\n",
    "When using CURL, the only thing we need to provide is the data in JSON format, as well as the url, which is of the format:\n",
    "\n",
    "```\n",
    "http://<ENDPOINT>/seldon/kubeflow/<PIPELINE_NAME>/api/v0.1/predictions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"meta\": {\n",
      "    \"puid\": \"k89krp6t7tfgb386nt6vc3iftk\",\n",
      "    \"tags\": {\n",
      "    },\n",
      "    \"routing\": {\n",
      "      \"cleantext\": -1,\n",
      "      \"tfidfvectorizer\": -1,\n",
      "      \"spacytokenizer\": -1\n",
      "    },\n",
      "    \"requestPath\": {\n",
      "      \"cleantext\": \"clean_text_transformer:0.1\",\n",
      "      \"tfidfvectorizer\": \"tfidf_vectorizer:0.1\",\n",
      "      \"lrclassifier\": \"lr_text_classifier:0.1\",\n",
      "      \"spacytokenizer\": \"spacy_tokenizer:0.1\"\n",
      "    },\n",
      "    \"metrics\": []\n",
      "  },\n",
      "  \"data\": {\n",
      "    \"names\": [\"t:0\", \"t:1\"],\n",
      "    \"ndarray\": [[0.6729318752883149, 0.3270681247116851]]\n",
      "  }\n",
      "}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100    72    0     0  100    72      0    354 --:--:-- --:--:-- --:--:--   356\r",
      "100   599  100   527  100    72    516     70  0:00:01  0:00:01 --:--:--   588\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -X POST -H 'Content-Type: application/json' \\\n",
    "    -d \"{'data': {'names': ['text'], 'ndarray': ['Hello world this is a test']}}\" \\\n",
    "    http://127.0.0.1/seldon/kubeflow/$(kubectl -n kubeflow get seldondeployment -o jsonpath='{.items[0].metadata.name}')/api/v0.1/predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the SeldonClient\n",
    "We can also use the Python SeldonClient to interact with the pipeline we just deployed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success:True message:\n",
      "Request:\n",
      "data {\n",
      "  names: \"text\"\n",
      "  ndarray {\n",
      "    values {\n",
      "      string_value: \"Hello world this is a test\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "Response:\n",
      "meta {\n",
      "  puid: \"qtdca40d3s0463nn4ginhkvc6t\"\n",
      "  routing {\n",
      "    key: \"cleantext\"\n",
      "    value: -1\n",
      "  }\n",
      "  routing {\n",
      "    key: \"spacytokenizer\"\n",
      "    value: -1\n",
      "  }\n",
      "  routing {\n",
      "    key: \"tfidfvectorizer\"\n",
      "    value: -1\n",
      "  }\n",
      "  requestPath {\n",
      "    key: \"cleantext\"\n",
      "    value: \"clean_text_transformer:0.1\"\n",
      "  }\n",
      "  requestPath {\n",
      "    key: \"lrclassifier\"\n",
      "    value: \"lr_text_classifier:0.1\"\n",
      "  }\n",
      "  requestPath {\n",
      "    key: \"spacytokenizer\"\n",
      "    value: \"spacy_tokenizer:0.1\"\n",
      "  }\n",
      "  requestPath {\n",
      "    key: \"tfidfvectorizer\"\n",
      "    value: \"tfidf_vectorizer:0.1\"\n",
      "  }\n",
      "}\n",
      "data {\n",
      "  names: \"t:0\"\n",
      "  names: \"t:1\"\n",
      "  ndarray {\n",
      "    values {\n",
      "      list_value {\n",
      "        values {\n",
      "          number_value: 0.6729318752883149\n",
      "        }\n",
      "        values {\n",
      "          number_value: 0.3270681247116851\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seldon_core.seldon_client import SeldonClient\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "host = \"localhost\"\n",
    "port = \"80\" # Make sure you use the port above\n",
    "batch = np.array([\"Hello world this is a test\"])\n",
    "payload_type = \"ndarray\"\n",
    "# Get the deployment name\n",
    "deployment_name = subprocess.getoutput(\"kubectl -n kubeflow get seldondeployment -o jsonpath='{.items[0].metadata.name}'\")\n",
    "transport=\"rest\"\n",
    "namespace=\"kubeflow\"\n",
    "\n",
    "sc = SeldonClient(\n",
    "    gateway=\"ambassador\", \n",
    "    ambassador_endpoint=host + \":\" + port,\n",
    "    namespace=namespace)\n",
    "\n",
    "client_prediction = sc.predict(\n",
    "    data=batch, \n",
    "    deployment_name=deployment_name,\n",
    "    names=[\"text\"],\n",
    "    payload_type=payload_type,\n",
    "    transport=\"rest\")\n",
    "\n",
    "print(client_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Visualise Seldon's Production ML Pipelines\n",
    "We can visualise the performance using the SeldonAnalytics package, which we can deploy using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!helm install seldon-core-analytics --repo https://storage.googleapis.com/seldon-charts --namespace kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my case, similar to what I did with Ambassador, I need to make sure the the service is a LoadBalancer instead of a NodePort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service/grafana-prom patched\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl patch svc grafana-prom --type='json' -p '[{\"op\":\"replace\",\"path\":\"/spec/type\",\"value\":\"LoadBalancer\"}]' -n kubeflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME           TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\r\n",
      "grafana-prom   LoadBalancer   10.98.248.223   localhost     80:32445/TCP   64m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get svc grafana-prom -n kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can access it at the port provided, in my case it is http://localhost:32445/d/3swM2iGWz/prediction-analytics?refresh=5s&orgId=1\n",
    "\n",
    "(initial username is admin and password is password, which will be requested to be changed on the first login)\n",
    "\n",
    "Generate a bunch of requests and visualise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    client_prediction = sc.predict(\n",
    "        data=batch, \n",
    "        deployment_name=deployment_name,\n",
    "        names=[\"text\"],\n",
    "        payload_type=payload_type,\n",
    "        transport=\"rest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You now have a full end-to-end training and production NLP pipeline ðŸ˜Ž \n",
    "![](img/seldon-analytics.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
